{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1868e173",
   "metadata": {},
   "source": [
    "# Apache Spark's DataFrame Usage\n",
    "\n",
    "In previous tutorials, we have seen the popularity of [Pandas](https://github.com/EQWorks/python-curriculum/blob/main/basics/03-data-containers-and-repetitions-3.md) as well as the [application of its functions](https://github.com/EQWorks/python-curriculum/blob/main/data-analysis/04-aggregation-and-visualization.ipynb). In this notebook, we will look at Apache Spark's library [`pyspark`](https://spark.apache.org/docs/latest/api/python/) with the scope of comparing basic uses through examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "combined-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkFiles\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb851de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4399bd5",
   "metadata": {},
   "source": [
    "## Loading & Viewing Data\n",
    "\n",
    "To load a dataset from a remote URL we can utilize `SparkContext.addFile()` and `SparkFiles.get()`. These functions allow for `pyspark` to resolve the absolute path of the data. Whereas in pandas we would simply put the URL into the file path of `.read_csv()` to be resolved.\n",
    "\n",
    "When comparing the display of a pandas dataframe to a `pyspark` dataframe, the difference is very apparent. \n",
    "There is a quick fix for displaying the data in a neater format by utilizing `.toPandas()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a150010",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/EQWorks/python-curriculum/main/data/sample_boards_report.csv'\n",
    "spark.sparkContext.addFile(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd452dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+------------+----------------------------------------+--------+---------+------+-----+----------+--------------+------------------------+--------------------+--------+-----------+--------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+--------+-------+--------+--------+--------+-----+-----+--------+-----+-----+--------+--------+-----+-----+--------+-----+-----+-----+-----+--------+-----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+-------+--------+\n",
      "|board_id|market |product_name|sales_address                           |lat     |long     |height|width|facing    |total_observed|audience_target_observed|audience_composition|index   |day_of_week|AD_00   |AD_01  |AD_02  |AD_03  |AD_04  |AD_05  |AD_06  |AD_07  |AD_08  |AD_09  |AD_10  |AD_11  |AD_12   |AD_13   |AD_14   |AD_15   |AD_16  |AD_17   |AD_18   |AD_19   |AD_20   |AD_21  |AD_22   |AD_23  |AC_00   |AC_01   |AC_02   |AC_03|AC_04|AC_05   |AC_06|AC_07|AC_08   |AC_09   |AC_10|AC_11|AC_12   |AC_13|AC_14|AC_15|AC_16|AC_17   |AC_18|AC_19   |AC_20   |AC_21   |AC_22   |AC_23   |I_00    |I_01    |I_02    |I_03    |I_04    |I_05    |I_06    |I_07   |I_08    |I_09    |I_10    |I_11    |I_12    |I_13    |I_14   |I_15    |I_16    |I_17    |I_18    |I_19    |I_20    |I_21    |I_22   |I_23    |\n",
      "+--------+-------+------------+----------------------------------------+--------+---------+------+-----+----------+--------------+------------------------+--------------------+--------+-----------+--------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+--------+-------+--------+--------+--------+-----+-----+--------+-----+-----+--------+--------+-----+-----+--------+-----+-----+-----+-----+--------+-----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+-------+--------+\n",
      "|7387802 |Toronto|Static      |HUMBER COLLEGE - MF-J-Block J107-M-WAL 1|43.72806|-79.60669|17    |13   |Main Floor|49            |26                      |0.530612            |0.791408|Wed        |0.056911|0.02439|0.02439|0.01626|0.01626|0.01626|0.00813|0.01626|0.01626|0.01626|0.01626|0.04065|0.065041|0.073171|0.081301|0.056911|0.04878|0.065041|0.081301|0.073171|0.056911|0.03252|0.065041|0.03252|0.466667|0.272727|0.272727|0.25 |0.4  |0.333333|0.2  |0.4  |0.333333|0.333333|0.4  |0.5  |0.571429|0.6  |0.625|0.5  |0.4  |0.470588|0.5  |0.529412|0.466667|0.363636|0.571429|0.307692|0.619849|0.373664|0.382388|0.354891|0.577593|0.489244|0.295994|0.58743|0.480632|0.461523|0.521109|0.636622|0.729806|0.763628|0.79739|0.653214|0.531563|0.616949|0.652551|0.685661|0.605912|0.471603|0.74584|0.407592|\n",
      "+--------+-------+------------+----------------------------------------+--------+---------+------+-----+----------+--------------+------------------------+--------------------+--------+-----------+--------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+--------+--------+-------+--------+--------+--------+--------+-------+--------+-------+--------+--------+--------+-----+-----+--------+-----+-----+--------+--------+-----+-----+--------+-----+-----+-----+-----+--------+-----+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+-------+--------+--------+--------+--------+--------+--------+--------+-------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Truncate is set to False, because we would like to see full column values\n",
    "spark.read.option(\"header\",True).csv(SparkFiles.get(\"sample_boards_report.csv\")).show(1, False)\n",
    "#.limit(1).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78a16955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>board_id</th>\n",
       "      <th>market</th>\n",
       "      <th>product_name</th>\n",
       "      <th>sales_address</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>facing</th>\n",
       "      <th>total_observed</th>\n",
       "      <th>...</th>\n",
       "      <th>I_14</th>\n",
       "      <th>I_15</th>\n",
       "      <th>I_16</th>\n",
       "      <th>I_17</th>\n",
       "      <th>I_18</th>\n",
       "      <th>I_19</th>\n",
       "      <th>I_20</th>\n",
       "      <th>I_21</th>\n",
       "      <th>I_22</th>\n",
       "      <th>I_23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7387802</td>\n",
       "      <td>Toronto</td>\n",
       "      <td>Static</td>\n",
       "      <td>HUMBER COLLEGE - MF-J-Block J107-M-WAL 1</td>\n",
       "      <td>43.72806</td>\n",
       "      <td>-79.60669</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Main Floor</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.79739</td>\n",
       "      <td>0.653214</td>\n",
       "      <td>0.531563</td>\n",
       "      <td>0.616949</td>\n",
       "      <td>0.652551</td>\n",
       "      <td>0.685661</td>\n",
       "      <td>0.605912</td>\n",
       "      <td>0.471603</td>\n",
       "      <td>0.74584</td>\n",
       "      <td>0.407592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   board_id   market product_name                             sales_address  \\\n",
       "0   7387802  Toronto       Static  HUMBER COLLEGE - MF-J-Block J107-M-WAL 1   \n",
       "\n",
       "        lat      long  height  width      facing  total_observed  ...  \\\n",
       "0  43.72806 -79.60669    17.0   13.0  Main Floor              49  ...   \n",
       "\n",
       "      I_14      I_15      I_16      I_17      I_18      I_19      I_20  \\\n",
       "0  0.79739  0.653214  0.531563  0.616949  0.652551  0.685661  0.605912   \n",
       "\n",
       "       I_21     I_22      I_23  \n",
       "0  0.471603  0.74584  0.407592  \n",
       "\n",
       "[1 rows x 86 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(url).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa631b",
   "metadata": {},
   "source": [
    "Another way to view the data is by looking at the schema. In Pandas we use `.dtypes()`) to show us the data type of each column.\n",
    "\n",
    "Unless the schema is specified while reading the file, the default for all columns will `string`. We can either rewrite the schema independently or add infer schema option when reading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05887ea3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- board_id: string (nullable = true)\n",
      " |-- market: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- sales_address: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      " |-- height: string (nullable = true)\n",
      " |-- width: string (nullable = true)\n",
      " |-- facing: string (nullable = true)\n",
      " |-- total_observed: string (nullable = true)\n",
      " |-- audience_target_observed: string (nullable = true)\n",
      " |-- audience_composition: string (nullable = true)\n",
      " |-- index: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- AD_00: string (nullable = true)\n",
      " |-- AD_01: string (nullable = true)\n",
      " |-- AD_02: string (nullable = true)\n",
      " |-- AD_03: string (nullable = true)\n",
      " |-- AD_04: string (nullable = true)\n",
      " |-- AD_05: string (nullable = true)\n",
      " |-- AD_06: string (nullable = true)\n",
      " |-- AD_07: string (nullable = true)\n",
      " |-- AD_08: string (nullable = true)\n",
      " |-- AD_09: string (nullable = true)\n",
      " |-- AD_10: string (nullable = true)\n",
      " |-- AD_11: string (nullable = true)\n",
      " |-- AD_12: string (nullable = true)\n",
      " |-- AD_13: string (nullable = true)\n",
      " |-- AD_14: string (nullable = true)\n",
      " |-- AD_15: string (nullable = true)\n",
      " |-- AD_16: string (nullable = true)\n",
      " |-- AD_17: string (nullable = true)\n",
      " |-- AD_18: string (nullable = true)\n",
      " |-- AD_19: string (nullable = true)\n",
      " |-- AD_20: string (nullable = true)\n",
      " |-- AD_21: string (nullable = true)\n",
      " |-- AD_22: string (nullable = true)\n",
      " |-- AD_23: string (nullable = true)\n",
      " |-- AC_00: string (nullable = true)\n",
      " |-- AC_01: string (nullable = true)\n",
      " |-- AC_02: string (nullable = true)\n",
      " |-- AC_03: string (nullable = true)\n",
      " |-- AC_04: string (nullable = true)\n",
      " |-- AC_05: string (nullable = true)\n",
      " |-- AC_06: string (nullable = true)\n",
      " |-- AC_07: string (nullable = true)\n",
      " |-- AC_08: string (nullable = true)\n",
      " |-- AC_09: string (nullable = true)\n",
      " |-- AC_10: string (nullable = true)\n",
      " |-- AC_11: string (nullable = true)\n",
      " |-- AC_12: string (nullable = true)\n",
      " |-- AC_13: string (nullable = true)\n",
      " |-- AC_14: string (nullable = true)\n",
      " |-- AC_15: string (nullable = true)\n",
      " |-- AC_16: string (nullable = true)\n",
      " |-- AC_17: string (nullable = true)\n",
      " |-- AC_18: string (nullable = true)\n",
      " |-- AC_19: string (nullable = true)\n",
      " |-- AC_20: string (nullable = true)\n",
      " |-- AC_21: string (nullable = true)\n",
      " |-- AC_22: string (nullable = true)\n",
      " |-- AC_23: string (nullable = true)\n",
      " |-- I_00: string (nullable = true)\n",
      " |-- I_01: string (nullable = true)\n",
      " |-- I_02: string (nullable = true)\n",
      " |-- I_03: string (nullable = true)\n",
      " |-- I_04: string (nullable = true)\n",
      " |-- I_05: string (nullable = true)\n",
      " |-- I_06: string (nullable = true)\n",
      " |-- I_07: string (nullable = true)\n",
      " |-- I_08: string (nullable = true)\n",
      " |-- I_09: string (nullable = true)\n",
      " |-- I_10: string (nullable = true)\n",
      " |-- I_11: string (nullable = true)\n",
      " |-- I_12: string (nullable = true)\n",
      " |-- I_13: string (nullable = true)\n",
      " |-- I_14: string (nullable = true)\n",
      " |-- I_15: string (nullable = true)\n",
      " |-- I_16: string (nullable = true)\n",
      " |-- I_17: string (nullable = true)\n",
      " |-- I_18: string (nullable = true)\n",
      " |-- I_19: string (nullable = true)\n",
      " |-- I_20: string (nullable = true)\n",
      " |-- I_21: string (nullable = true)\n",
      " |-- I_22: string (nullable = true)\n",
      " |-- I_23: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option(\"header\",True).csv(SparkFiles.get(\"sample_boards_report.csv\")).printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80565c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- board_id: integer (nullable = true)\n",
      " |-- market: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- total_observed: integer (nullable = true)\n",
      " |-- audience_target_observed: integer (nullable = true)\n",
      " |-- audience_composition: double (nullable = true)\n",
      " |-- index: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSpark = spark.read.option(\"header\",True).option(\"inferSchema\", True).csv(SparkFiles.get(\"sample_boards_report.csv\"))\n",
    "# selecting columns for better viewing\n",
    "dfSpark = dfSpark.select('board_id','market','day_of_week','product_name','total_observed','audience_target_observed', 'audience_composition', 'index')\n",
    "dfSpark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e21703d",
   "metadata": {},
   "source": [
    "We have previously seen and heard of how the `Pandas` library offers many tools for data grouping, aggregation, reshaping, filtering, reformatting, etc. in tutorials such as [03-data-containers-and-repetitions-3](https://github.com/EQWorks/python-curriculum/blob/294b55343b469152463f920086bd2876fd282a21/basics/03-data-containers-and-repetitions-3.md) and [04-aggregation-and-visualization](https://github.com/EQWorks/python-curriculum/blob/main/data-analysis/04-aggregation-and-visualization.ipynb). In this section, we will continue exploring the offerings of the `pyspark` library to a similar extent, and how the library can be utilized interchangeably with Pandas to process data. We will also further examine the implementation of [SQL](https://github.com/EQWorks/python-curriculum/blob/main/data-analysis/11-work-with-sql.ipynb) in Spark. We will see a similar implementation of keyword use to process and examine data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51746b32",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "\n",
    "As we continue with these examples, a strong resemblance to SQL-style coding will be prominent. \n",
    "```\n",
    "    SELECT (product_name, total_observed, ...) FROM Table\n",
    "```\n",
    "\n",
    "In this first example we will use the keyword `select` to choose the columns we want displayed. \n",
    "\n",
    "In pandas, we would choose columns by implementing indexing with a column list e.g. `df[[column name list]]`\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a1901b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+------------------------+--------------------+--------+\n",
      "|product_name     |total_observed|audience_target_observed|audience_composition|index   |\n",
      "+-----------------+--------------+------------------------+--------------------+--------+\n",
      "|Static           |49            |26                      |0.530612            |0.791408|\n",
      "|Classique/Classic|63            |36                      |0.571429            |0.843304|\n",
      "|Static           |54            |31                      |0.574074            |0.865937|\n",
      "|Static           |183           |106                     |0.579235            |1.077622|\n",
      "|Static           |48            |34                      |0.708333            |1.056479|\n",
      "+-----------------+--------------+------------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Filter = dfSpark.select('product_name','total_observed','audience_target_observed', 'audience_composition', 'index')\n",
    "Filter.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f6f0b",
   "metadata": {},
   "source": [
    "The equivalent of `.unique()` in pandas. \n",
    "\n",
    "Again we see keywords i.e. `select`, `distinct`, `collect`, similar to querying data in dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf1362d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(product_name='Bike Share'),\n",
       " Row(product_name='Transit Shelter'),\n",
       " Row(product_name='Horizontal'),\n",
       " Row(product_name='Classique/Classic'),\n",
       " Row(product_name='Static'),\n",
       " Row(product_name='Bike Share / VÃ©los en ville'),\n",
       " Row(product_name='Signature Column/Colonne Signature'),\n",
       " Row(product_name='Column')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Filter.select('product_name').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07935d65",
   "metadata": {},
   "source": [
    "Filtering would be implemented similarily to Pandas by index `df[condition]`. We would use `&` `|` `~` to represent `AND`, `OR`, and `NOT`, instead of using the keyword as they would in a query. \n",
    "\n",
    "```\n",
    "SELECT *\n",
    "    FROM poi_lists\n",
    "    WHERE name LIKE '%pizza%'\n",
    "        OR name LIKE '%domino%'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2537796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+------------------------+--------------------+--------+\n",
      "|product_name|total_observed|audience_target_observed|audience_composition|index   |\n",
      "+------------+--------------+------------------------+--------------------+--------+\n",
      "|Static      |49            |26                      |0.530612            |0.791408|\n",
      "+------------+--------------+------------------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Filter.filter((Filter.product_name == 'Static') & (Filter.total_observed == 49)).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f487d6",
   "metadata": {},
   "source": [
    "## Creating New Columns\n",
    "\n",
    "Keyword: `withColumn` \n",
    "\n",
    "Need to reference column names as `col(column name)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04007d1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+------------------------+--------------------+--------+-------------+\n",
      "|product_name     |total_observed|audience_target_observed|audience_composition|index   |calculated_AC|\n",
      "+-----------------+--------------+------------------------+--------------------+--------+-------------+\n",
      "|Static           |49            |26                      |0.530612            |0.791408|0.530612     |\n",
      "|Classique/Classic|63            |36                      |0.571429            |0.843304|0.571429     |\n",
      "|Static           |54            |31                      |0.574074            |0.865937|0.574074     |\n",
      "|Static           |183           |106                     |0.579235            |1.077622|0.579235     |\n",
      "|Static           |48            |34                      |0.708333            |1.056479|0.708333     |\n",
      "+-----------------+--------------+------------------------+--------------------+--------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Filter.withColumn('calculated_AC', round(col('audience_target_observed')/col('total_observed'), 6)).show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb041a2",
   "metadata": {},
   "source": [
    "## Merging DataFrames\n",
    "\n",
    "As seen in [11-work-with-sql](https://github.com/EQWorks/python-curriculum/blob/main/data-analysis/11-work-with-sql.ipynb), the comparison between Pandas `merge()` function and SQL `joins` can achieve similar effects.\n",
    "\n",
    "And do to its relation to using SQL functions, `join()` is the function to reproduce the effects of Pandas `merge()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24f5736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------------+-----------------+--------------------+--------+--------+-------+-----------+-----------------+--------------------+--------+\n",
      "|total_observed|audience_target_observed|product_name     |audience_composition|index   |board_id|market |day_of_week|product_name     |audience_composition|index   |\n",
      "+--------------+------------------------+-----------------+--------------------+--------+--------+-------+-----------+-----------------+--------------------+--------+\n",
      "|49            |26                      |Static           |0.530612            |0.791408|7387802 |Toronto|Wed        |Static           |0.530612            |0.791408|\n",
      "|63            |36                      |Classique/Classic|0.571429            |0.843304|7387802 |Toronto|Tue        |Classique/Classic|0.571429            |0.843304|\n",
      "|54            |31                      |Static           |0.574074            |0.865937|7387802 |Toronto|Fri        |Static           |0.574074            |0.865937|\n",
      "|183           |106                     |Static           |0.579235            |1.077622|7387802 |Toronto|total      |Static           |0.579235            |1.077622|\n",
      "|48            |34                      |Static           |0.708333            |1.056479|7395463 |Toronto|Wed        |Static           |0.708333            |1.056479|\n",
      "+--------------+------------------------+-----------------+--------------------+--------+--------+-------+-----------+-----------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Filter.join(dfSpark, on=['total_observed','audience_target_observed'], how='inner').show(5, False)\n",
    "#.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ec658",
   "metadata": {},
   "source": [
    "## GroupBys\n",
    "\n",
    "We see another concept from [11-work-with-sql](https://github.com/EQWorks/python-curriculum/blob/main/data-analysis/11-work-with-sql.ipynb), Groupbys. The concept of groupby is quite similar to that of Pandas `.groupby()`, even with the inclusion of aggregating multiple stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12344fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----------------+\n",
      "|product_name                      |sum(index)       |\n",
      "+----------------------------------+-----------------+\n",
      "|Bike Share                        |5.392304         |\n",
      "|Transit Shelter                   |75.97666600000001|\n",
      "|Horizontal                        |1.28497          |\n",
      "|Classique/Classic                 |0.843304         |\n",
      "|Static                            |6.441741         |\n",
      "|Bike Share / VÃ©los en ville       |1.316308         |\n",
      "|Signature Column/Colonne Signature|1.245631         |\n",
      "|Column                            |1.580411         |\n",
      "+----------------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSpark.groupBy(\"product_name\").sum(\"index\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e77f8d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----------------+------------------+\n",
      "|product_name                      |index_total      |avg_ac            |\n",
      "+----------------------------------+-----------------+------------------+\n",
      "|Bike Share                        |5.392304         |0.8032934999999999|\n",
      "|Transit Shelter                   |75.97666600000001|0.8458088070175439|\n",
      "|Horizontal                        |1.28497          |0.854839          |\n",
      "|Classique/Classic                 |0.843304         |0.571429          |\n",
      "|Static                            |6.441741         |0.6619383333333334|\n",
      "|Bike Share / VÃ©los en ville       |1.316308         |0.896552          |\n",
      "|Signature Column/Colonne Signature|1.245631         |0.842466          |\n",
      "|Column                            |1.580411         |0.84949           |\n",
      "+----------------------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfSpark.groupBy(\"product_name\").agg(sum(\"index\").alias('index_total'),\n",
    "                                    avg(\"audience_composition\").alias('avg_ac')).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba25b9b2",
   "metadata": {},
   "source": [
    "## Creating user defined functions\n",
    "\n",
    "We can create functions as we would normally `def function(dependents): ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d3df8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.avg_ac(a, t)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg_ac(a, t):\n",
    "    \n",
    "    return a/t\n",
    "\n",
    "# Registers function to allow user to preset return schema\n",
    "spark.udf.register(\"Avg_AC\", avg_ac, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e17a419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------+------------------------+--------------------+--------+------------------+\n",
      "|product_name     |total_observed|audience_target_observed|audience_composition|index   |function_test     |\n",
      "+-----------------+--------------+------------------------+--------------------+--------+------------------+\n",
      "|Static           |49            |26                      |0.530612            |0.791408|0.5306122448979592|\n",
      "|Classique/Classic|63            |36                      |0.571429            |0.843304|0.5714285714285714|\n",
      "|Static           |54            |31                      |0.574074            |0.865937|0.5740740740740741|\n",
      "|Static           |183           |106                     |0.579235            |1.077622|0.5792349726775956|\n",
      "|Static           |48            |34                      |0.708333            |1.056479|0.7083333333333334|\n",
      "+-----------------+--------------+------------------------+--------------------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Filter.withColumn(\"function_test\", avg_ac(col('audience_target_observed'),col('total_observed')).alias(\"calc_ac_test\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52081216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
