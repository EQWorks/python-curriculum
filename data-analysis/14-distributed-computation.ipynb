{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12adacc-007f-40fb-a86e-37ec50ac2b6f",
   "metadata": {},
   "source": [
    "# Distributed Computation\n",
    "\n",
    "## Quick Recap - Multiprocessing vs. Multithreading\n",
    "\n",
    "Multiprocessing can utilize multiple CPU cores, thus achieving a more real sense of a parallel computation. However, multiprocessing suffers when they need to share a common memory space.\n",
    "\n",
    "On the other hand, multithreading within a process can share a common memory space, and achieve a more loose sense of a parallel computation. Multithreading is more like hyper-looping through multiple queues while waiting for each particular thread's turn; and when it is a particular thread's turn, it would acquire the Global Interpreter Lock (GIL) to hog the memory space _and_ the CPU core the entire process runs on, until it finishes its designated computation or until it hits another \"busy-waiting\" block such as some I/O action, where the loop releases the GIL and let the jump proceed (context-switch) to the next thread.\n",
    "\n",
    "### Asynchronous I/O Loops, and Multithreading's Little Brother - Coroutines\n",
    "\n",
    "Both multiprocessing and multithreading require dedicated hardware and operating system coordination. As software technologies mature, engineers started to explore capabilities within the application layer itself. The same conceptual model of multithreading (again, the hyper-looping) gets a new interpretation within programming stack (such as Python, or more appropriately the CPython runtime), giving more direct control to the program within the runtime instead of relying on the OS mechanism to switch context between threads. The term _Coroutine_, first coined in 1958, is such a materialization of the concept that can be seen as lightweight threads, and has seen its implementation in many languages including Python. Its implementation usually involves dedicated asynchronous I/O loops (or event loops) to switch between the coroutines with more direct controls than threading on when to suspend and resume each task.\n",
    "\n",
    "Both multithreading and coroutine techniques are suitable for I/O focused tasks, such as reading files from a disk, or making HTTP requests. Coroutines tend to require less computing resource overhead compared to its more OS-native sibling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62e3a796-c985-4659-a933-2d4dde6a1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "\n",
    "\n",
    "# 5 requests, each with delays ranging from 1-5 seconds\n",
    "reqs = [\n",
    "    f'https://httpbin.org/delay/{random.randint(1, 3)}'\n",
    "    for _ in range(10)\n",
    "]\n",
    "\n",
    "def get_sync():\n",
    "    all_data = []\n",
    "    for req in reqs:\n",
    "        res = requests.get(req)\n",
    "        all_data.append(res.json())\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f8483a5c-e857-41d8-8191-14e5ff96132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 163 ms, sys: 12.9 ms, total: 176 ms\n",
      "Wall time: 23.6 s\n",
      "{'args': {}, 'data': '', 'files': {}, 'form': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.25.1', 'X-Amzn-Trace-Id': 'Root=1-60d10ac9-550eb0d6203262b369bd3576'}, 'origin': '107.179.188.69', 'url': 'https://httpbin.org/delay/2'} 10\n"
     ]
    }
   ],
   "source": [
    "%time res = get_sync()\n",
    "print(res[-1], len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b010385-d4ff-4870-9e0e-4f1164dc465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import aiohttp\n",
    "\n",
    "\n",
    "async def get(session, url):\n",
    "    res = await session.request('GET', url=url)\n",
    "    data = await res.json()\n",
    "    return data\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for req in reqs:\n",
    "            tasks.append(get(session, url=req))\n",
    "\n",
    "        all_data = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f338e2f-1951-4558-bf6d-531bd7becb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2 seconds\n",
      "{'args': {}, 'data': '', 'files': {}, 'form': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'Python/3.8 aiohttp/3.7.4.post0', 'X-Amzn-Trace-Id': 'Root=1-60d10acc-6bdccf035163f7cc14d45527'}, 'origin': '107.179.188.69', 'url': 'https://httpbin.org/delay/2'} 10\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Two things about Jupyter Notebook environment\n",
    "1. Cannot use %time magic for async functions\n",
    "2. No explicit event loop initiation (it's already in one)\n",
    "'''\n",
    "import time\n",
    "\n",
    "# loop = asyncio.get_event_loop()\n",
    "# loop.run_until_complete(main())\n",
    "start = time.time()\n",
    "res = await main()\n",
    "print(round(time.time() - start, 1), 'seconds')\n",
    "print(res[-1], len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e866076f-3616-4e9b-8b48-2c07c6f5f627",
   "metadata": {},
   "source": [
    "Let's use the responses to verify what the delays were involved in the API calls, by using functional techniques `map()` and `reduce()` to extract delays (in seconds) and sum them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "920e2588-47d2-44a4-a979-2f14739fe203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 2, 3, 3, 2, 3, 2, 2]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delays = [*map(lambda url: int(url.split('/')[-1]), reqs)]\n",
    "delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "658e2b6b-5d19-44eb-a486-b6841d3bf1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "total_delay = reduce(lambda left, right: left + right, [*delays], 0)\n",
    "total_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba3091-88b4-46c6-a431-09afb9da6873",
   "metadata": {},
   "source": [
    "The theoretical total delay matches our observation from the synchronous process, while the _maximum_ from individual delays matches our observation from the asynchronous process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12faf4-d0ea-4921-b5e1-01a864f7aa8b",
   "metadata": {},
   "source": [
    "## MapReduce - Distributing Computing Resources\n",
    "\n",
    "Recall one of the most critical disadvantages that multiprocessing - the lack of a shared memory space. This particular constraint makes attempt to perform parrallel computation across multiple cores on a single machine often less desirable. One workaround of such issue is to leverage a datastore (such as files or databases backed by harddrives) as an inter-process data pool so that multiple processes can simultaneously read and write to it, such as a local SQLite database, as demonstrated in [a previous part](./11-work-with-sql.ipynb).\n",
    "\n",
    "If we extend this problem to a larger scale, where not only the dataset we want to work with exceed the available memory space, but also impossible to efficiently store (or at all) on the disk of a single machine, then we need to revisit viable solutions.\n",
    "\n",
    "The MapReduce model is a _divide and conquer_ strategy applying and extending the functional programming concepts of `map()` and `reduce()`, where a large dataset is dissected and distributed through a mapping procedure onto a multitude of \"commodity\" server nodes to parrallize the computation of the smaller portion, then reducing the resulting subsets back to less and less nodes until the final outcome is completed.\n",
    "\n",
    "This model allows batch data processing to have near infinite capacity, and a relatively cost-effective way to speed up the process.\n",
    "\n",
    "It is worth knowing that the recent development of cheaper and faster harddrives, especially the more wide-spread adoption of SSDs (solid-state drive), plays a vital role in enabling distributed computation.\n",
    "\n",
    "The MapReduce model was first pioneered [by Google](https://research.google/pubs/pub62/) in 2004 to resolve the practical problem of exponentially growing dataset for computing their search indexes. Then many have adopted and contributed toward the technology's development and evolution through the open-source community.\n",
    "\n",
    "### Apache Spark™\n",
    "\n",
    "Apache Spark is such an open-source framework that came around 2014 (10 years after the initial MapReduce research paper) that provides an elegant and unified abstraction to enable large-scale data processing that can efficiently utilize from multiple-cores of a single machine to \"multiple-clouds\".\n",
    "\n",
    "For the sake of simplicity, we will demonstrate through its usage on a single machine. First, obtain the number of CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2d71cc-fdac-4c6e-985f-8a7f784b37a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "CORES = multiprocessing.cpu_count()\n",
    "CORES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ec5e4-c646-4357-b372-78c5d1d4e8e7",
   "metadata": {},
   "source": [
    "Borrowing from [another previous part](./12-generate-data.ipynb), where we attempted to generate random and hashed device IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab89d25-26a1-4bf9-841b-23005c22b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostly from Part 12 - Generate data\n",
    "from uuid import uuid4\n",
    "from hashlib import sha1\n",
    "\n",
    "\n",
    "def gen_device_ids(count: int = 20_000) -> list:\n",
    "    device_ids = []\n",
    "    for _ in range(count):\n",
    "        device_ids.append(str(uuid4()))\n",
    "    # hash\n",
    "    return [sha1(x.encode()).hexdigest() for x in device_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d5ea06-9dba-44f2-a653-9c2856366b4d",
   "metadata": {},
   "source": [
    "Let's generate a relatively large batch of device IDs, say 1 million times the number of `CORES`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a7c6a45-9867-4cff-9f28-c58b3025ac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.3 s, sys: 5.25 s, total: 57.5 s\n",
      "Wall time: 57.7 s\n"
     ]
    }
   ],
   "source": [
    "%time device_ids = gen_device_ids(count=1_000_000 * CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8c3ce-3642-46b8-800b-418b0c753344",
   "metadata": {},
   "source": [
    "Due to the iterative single process, the time it takes is quite long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a5abe5-5615-46c8-947d-e04def2c91cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 6 µs, total: 10 µs\n",
      "Wall time: 12.9 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['79e0a59c3636da4fc3dfbe1856224cf46e76a3bf',\n",
       " '26342eaf9e59e8ccf94e272d2bc37ace8e8fa603',\n",
       " '7698ecca8a088d35cc87f059bba69eaa2639b486',\n",
       " 'af785b16f54542341b591fab8fc73283b481dff4',\n",
       " '0756f3791e74466205d241a8f5fcdba67a746bc8',\n",
       " '55e152016fe246f7d158428025d2a179ff44167f',\n",
       " 'a7eefe3a2abbd91cf8fa68d587f340b91184c6d4',\n",
       " 'a4d234f1f9fa2b3994f9160b762f5e7edadb8604',\n",
       " '529baede08b092c9496eb301052da9819419418f',\n",
       " '2dc7f022908dad5f2c717fad0d4c27cc141caef3']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time device_ids[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481733f-e3f3-4678-a2d9-7af6a53e8b54",
   "metadata": {},
   "source": [
    "But since the list is already in memory, it takes very little time to read them out.\n",
    "\n",
    "Let's try the same device ID generation with Spark in a distributed manner. We will initiate a Spark session with a local \"master\" that takes advantage of the number of `CORES` we have obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30282e77-5efc-45fb-8753-e1a006c4051c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://runzhoudembp.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[12]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10d8a0880>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(f'local[{CORES}]').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d1fb0-1a29-45a8-a632-945ed4a6623a",
   "metadata": {},
   "source": [
    "The Spark UI which runs on the `localhost` is handy for monitoring and more.\n",
    "\n",
    "![Spark UI](https://user-images.githubusercontent.com/2837532/122995953-46a9c700-d378-11eb-84a7-50917d34b7be.png)\n",
    "\n",
    "The first approach involves a core concept and building block of Spark which is known as Resilient Distributed Datasets (`RDD`s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb80f2ce-34df-4ab1-b043-4e6585ada283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.84 ms, sys: 1.99 ms, total: 3.82 ms\n",
      "Wall time: 238 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def mapper(count):\n",
    "    return [(d,) for d in gen_device_ids(count)]\n",
    "\n",
    "# a list of [1_000_000, ..., 1_000_000], where the length is the number of CORES\n",
    "counts = [1_000_000] * CORES\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(counts).flatMap(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e05c8-c9ee-46e5-8301-0f3dbe2c15b5",
   "metadata": {},
   "source": [
    "The action takes a context from the available Spark session, schedule to parallelize over a list of 1 million counts, where the list length is the number of `CORES`. Then we instruct the parallelization to map the list over a `mapper` function that takes the individual count and generate a list of device IDs (in the form of a single element tuple). The `.flatMap` method is a convenient layer to ensure the resulting dataset is flattened as a single-tier list of device IDs, instead of a list of (number of `CORES`) lists.\n",
    "\n",
    "Notice the time it takes to schedule is negligible, as Spark takes a _lazy_ approach to preserve computing resources until the computation is really needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0df7f70-b1ee-44fe-af21-89c21df0221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.76 ms, sys: 2.12 ms, total: 5.88 ms\n",
      "Wall time: 5.08 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('79993454cc645c0c78569b8fe98b80cd9df4828f',),\n",
       " ('b164eaec7921780e336661d70215649de2a4eb09',),\n",
       " ('63d1760e28667a727e502b7a86c98bcfe25f7714',),\n",
       " ('54154a58b3d557dca36f9970d5d158ccc74a155f',),\n",
       " ('bb2e375ce4ffa9e7caa5ccf641e0bbc05147cb33',),\n",
       " ('8e2a7429109c13451ce9bfe55f6ce6878cf584b1',),\n",
       " ('d6d74a1717aa4dff0e7dd29976182c4b3d3d689e',),\n",
       " ('5e1010d089f18701dd96bccaae9d764290bc4a6b',),\n",
       " ('b9e5d165e2011bac5f848282e017c7c2cfcd59fc',),\n",
       " ('d6118fe0b844013c57416360c6418822a139a4ad',)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22793df6-78be-48e4-8b5c-87964f7230fc",
   "metadata": {},
   "source": [
    "Indeed, when we instruct to take the first 10 values from the RDD, Spark would actually start the computation (and more). If we go back to the Spark UI, it may paint a better picture on what Spark has done under the hood.\n",
    "\n",
    "![RDD take](https://user-images.githubusercontent.com/2837532/122997519-16fbbe80-d37a-11eb-8704-5c520019e161.png)\n",
    "\n",
    "Conceptually, the workflow is as the following:\n",
    "1. Generate the device IDs leveraging multiple `CORES` in parallel.\n",
    "2. Persist device IDs as RDD into distributed blocks on disk. They are also replicated automatically across available distributions to support further parallelized processes. Spark also records other necessary metadata such as order of values to ensure that computations that require strict ordering do not get affected.\n",
    "3. Draw the first 10 values from the RDD files and populates into the Python process that runs this Notebook.\n",
    "\n",
    "This means that while it is significantly faster to generate the device IDs compare to the single process iterative approach, it is also much slower to read it out as it involves a more complex trip to read from files while ensuring the order of values are intact.\n",
    "\n",
    "Spark also has a built-in DataFrame implementation on top of the RDDs, providing neat abstractions such as SQL queries and more, similar to Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79918937-4a6b-43ee-a35a-ad09406581d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 192 ms, sys: 53.9 ms, total: 246 ms\n",
      "Wall time: 7.04 s\n"
     ]
    }
   ],
   "source": [
    "%time df = rdd.toDF(['device_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80978d82-77ca-4c72-a07e-3d107e6208c3",
   "metadata": {},
   "source": [
    "This step reveals the trade-off more prominently, where the time it takes to operate on distributed dataset can be much more time consuming than its in-memory counterpart, where we can take the in-memory `list` of `device_ids` and convert it into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14282bfc-a1f2-41a7-8e23-95a6e81a0804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 435 ms, sys: 26.8 ms, total: 462 ms\n",
      "Wall time: 460 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79e0a59c3636da4fc3dfbe1856224cf46e76a3bf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26342eaf9e59e8ccf94e272d2bc37ace8e8fa603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7698ecca8a088d35cc87f059bba69eaa2639b486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af785b16f54542341b591fab8fc73283b481dff4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0756f3791e74466205d241a8f5fcdba67a746bc8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999995</th>\n",
       "      <td>e2e668530153491480fd72d202b7c1cfc67eef9d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999996</th>\n",
       "      <td>e5cbb4ffaf8df7c646dea909adcecdaf3683ea3d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999997</th>\n",
       "      <td>5ef67c9422a65985d5b15e838fe7898dc1f638c6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999998</th>\n",
       "      <td>3171ae0274118bd36060f6d69be2beba6283d9f4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999999</th>\n",
       "      <td>99e2a0ebd69e5d989ebf816686a78f9b0c7fdb38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         device_id\n",
       "0         79e0a59c3636da4fc3dfbe1856224cf46e76a3bf\n",
       "1         26342eaf9e59e8ccf94e272d2bc37ace8e8fa603\n",
       "2         7698ecca8a088d35cc87f059bba69eaa2639b486\n",
       "3         af785b16f54542341b591fab8fc73283b481dff4\n",
       "4         0756f3791e74466205d241a8f5fcdba67a746bc8\n",
       "...                                            ...\n",
       "11999995  e2e668530153491480fd72d202b7c1cfc67eef9d\n",
       "11999996  e5cbb4ffaf8df7c646dea909adcecdaf3683ea3d\n",
       "11999997  5ef67c9422a65985d5b15e838fe7898dc1f638c6\n",
       "11999998  3171ae0274118bd36060f6d69be2beba6283d9f4\n",
       "11999999  99e2a0ebd69e5d989ebf816686a78f9b0c7fdb38\n",
       "\n",
       "[12000000 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%time pdf = pd.DataFrame(device_ids, columns=['device_id'])\n",
    "\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80250bd-5186-48cc-8fa1-d68d777000dc",
   "metadata": {},
   "source": [
    "Bearing the same understanding, the Spark DataFrame would be slower to read, due to the round-trip it takes to the distributed disk blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eb420e0-3da9-438a-ba00-769fd74c20bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           device_id|\n",
      "+--------------------+\n",
      "|ecfb6dcdf0bd44198...|\n",
      "|5a1d3bfcb6cfc790a...|\n",
      "|5438b4a97af6e1145...|\n",
      "|5835e3a3659e2b7a8...|\n",
      "|ed8503d8c602f0186...|\n",
      "|65d960080f7fc74a4...|\n",
      "|6ae28893e34a562bc...|\n",
      "|5d3c7e681d93779f1...|\n",
      "|c56a2de5130910ea1...|\n",
      "|aea15eedf19160578...|\n",
      "|c2a80b65a38f343a7...|\n",
      "|f215008be624bb4b8...|\n",
      "|7b03ce84ed2b2e7f1...|\n",
      "|c9c7ed1893d923738...|\n",
      "|7255f6af353da3b1b...|\n",
      "|2cbcf97407d407a96...|\n",
      "|e980b98cc073b8162...|\n",
      "|7ae965b3d9e6f3671...|\n",
      "|4794302bc6369338d...|\n",
      "|eb31369d58bd79439...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 1.17 ms, sys: 1.15 ms, total: 2.32 ms\n",
      "Wall time: 5.72 s\n"
     ]
    }
   ],
   "source": [
    "%time df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e93dcb6-db18-42ec-b2ba-b7773b3c00b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 371 ms, sys: 8.01 ms, total: 379 ms\n",
      "Wall time: 378 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device_id    12000000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "418fede8-ede8-4edc-ad5d-db1c7e198864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 ms, sys: 1.8 ms, total: 4.08 ms\n",
      "Wall time: 16.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe9ebd-f9db-4f97-960f-25eecd7d328c",
   "metadata": {},
   "source": [
    "Taking the full count can be more significant of a difference. Spark would attempt to perform underlying optimization to not scan the full range of dataset when invoking partial readings such as `rdd.take()` or `df.show()`, but suffers a full-range scan when it needs to count the accurate number of items, hence the much longer duration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271355d8-1cf8-4a91-8a3f-8b64f3904888",
   "metadata": {},
   "source": [
    "Spark DataFrames can also be created directly, or through the help of concatinating existing Pandas DataFrames.\n",
    "\n",
    "Below is an alternative approach to perform the device ID generation task by leveraging the robust support of Spark's native support to interface with Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bca3108-7df8-4744-adcb-26a2e221a769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.9 ms, sys: 9.64 ms, total: 57.6 ms\n",
      "Wall time: 286 ms\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(i,) for i in range(CORES)], ['cluster'])\n",
    "\n",
    "def _gen(df):\n",
    "    device_ids = gen_device_ids(count=1_000_000)\n",
    "    pdf = pd.DataFrame(device_ids, columns=['device_id'])\n",
    "    pdf['cluster'] = df['cluster']\n",
    "    return pdf.reset_index()\n",
    "\n",
    "def gen_device_ids_udf(df):\n",
    "    output = []\n",
    "    for _, row in df.iterrows():\n",
    "        pdf = _gen(df)\n",
    "        output.append(pdf)\n",
    "\n",
    "    return pd.concat(output)\n",
    "\n",
    "\n",
    "schema = 'index long, cluster long, device_id string'\n",
    "%time df = df.groupby('cluster').applyInPandas(gen_device_ids_udf, schema=schema).drop('cluster', 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe3c37-9cf6-439a-8b57-fac23ba580f5",
   "metadata": {},
   "source": [
    "The similar parallelization principle from the RDD approach applies here, with a _hack_ around the mechanism of `.groupby()` which enables the parallelization against the number fo `CORES`.\n",
    "\n",
    "Also similarly, the scheduling of the task does not take much, as we have yet to request for actual access to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d620162-9d26-46d5-871d-82b987b73541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           device_id|\n",
      "+--------------------+\n",
      "|5638653346e9ce267...|\n",
      "|07f8429ab6091e2c9...|\n",
      "|a2da510584b0458ed...|\n",
      "|208c9da15db43f49f...|\n",
      "|372146c8becb3d98e...|\n",
      "|fe12fb491578ede7c...|\n",
      "|11b48aa52f8270b31...|\n",
      "|a49a66f75b9f9a337...|\n",
      "|5ad8e84bd2eb1f078...|\n",
      "|4d90b6f6b2464b593...|\n",
      "|0b6776512b4375db1...|\n",
      "|6b052b60b09248814...|\n",
      "|b76fe3337f457ce6c...|\n",
      "|786be5ba9c10f7505...|\n",
      "|7e81fcc65b76b9fa9...|\n",
      "|2c8ec661fce7d2923...|\n",
      "|2ca0c05a27bc9de66...|\n",
      "|7bf4786e3241d37ed...|\n",
      "|c252efa35cf5b4045...|\n",
      "|2bafd46820109a91f...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 1.14 ms, sys: 1.25 ms, total: 2.39 ms\n",
      "Wall time: 5.79 s\n"
     ]
    }
   ],
   "source": [
    "%time df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab32a57-cb9e-4c90-b769-0a5cdfd43f1d",
   "metadata": {},
   "source": [
    "Since this approach involves a more pronounced mapping process abstracted by the `.groupby` method, underlying workflow can be a bit more interesting to observe:\n",
    "\n",
    "![group by](https://user-images.githubusercontent.com/2837532/123001100-5b895900-d37e-11eb-8b2a-db3fa40caaba.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5144c54a-b74a-43d4-94fc-953e27d84f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.35 ms, sys: 2.02 ms, total: 4.38 ms\n",
      "Wall time: 18.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908a266-8738-43b2-b96f-22cf114c11d8",
   "metadata": {},
   "source": [
    "Read access and the time consumption behaviours are within our expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c4298-1a42-426d-a252-8466de901e80",
   "metadata": {},
   "source": [
    "Let's attempt to perform some actual analysis of the overall dataset, such as counting the number of device IDs by their first characters.\n",
    "\n",
    "We'll start with the Pandas DataFrame, which is in-memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d87e7dc-8cb7-4dfc-8810-993e34d40848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79e0a59c3636da4fc3dfbe1856224cf46e76a3bf</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26342eaf9e59e8ccf94e272d2bc37ace8e8fa603</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7698ecca8a088d35cc87f059bba69eaa2639b486</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af785b16f54542341b591fab8fc73283b481dff4</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0756f3791e74466205d241a8f5fcdba67a746bc8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999995</th>\n",
       "      <td>e2e668530153491480fd72d202b7c1cfc67eef9d</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999996</th>\n",
       "      <td>e5cbb4ffaf8df7c646dea909adcecdaf3683ea3d</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999997</th>\n",
       "      <td>5ef67c9422a65985d5b15e838fe7898dc1f638c6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999998</th>\n",
       "      <td>3171ae0274118bd36060f6d69be2beba6283d9f4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999999</th>\n",
       "      <td>99e2a0ebd69e5d989ebf816686a78f9b0c7fdb38</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         device_id first\n",
       "0         79e0a59c3636da4fc3dfbe1856224cf46e76a3bf     7\n",
       "1         26342eaf9e59e8ccf94e272d2bc37ace8e8fa603     2\n",
       "2         7698ecca8a088d35cc87f059bba69eaa2639b486     7\n",
       "3         af785b16f54542341b591fab8fc73283b481dff4     a\n",
       "4         0756f3791e74466205d241a8f5fcdba67a746bc8     0\n",
       "...                                            ...   ...\n",
       "11999995  e2e668530153491480fd72d202b7c1cfc67eef9d     e\n",
       "11999996  e5cbb4ffaf8df7c646dea909adcecdaf3683ea3d     e\n",
       "11999997  5ef67c9422a65985d5b15e838fe7898dc1f638c6     5\n",
       "11999998  3171ae0274118bd36060f6d69be2beba6283d9f4     3\n",
       "11999999  99e2a0ebd69e5d989ebf816686a78f9b0c7fdb38     9\n",
       "\n",
       "[12000000 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf['first'] = pdf.device_id.astype(str).str[0]\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6dd9960-f20e-4bd8-9d6e-adbc6a3d74c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12000000</td>\n",
       "      <td>2.448123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>0.029682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200</td>\n",
       "      <td>0.002271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.004389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       size      time\n",
       "0  12000000  2.448123\n",
       "1    120000  0.029682\n",
       "2      1200  0.002271\n",
       "3        12  0.004389"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "size_time = []\n",
    "size = len(pdf)\n",
    "\n",
    "while size > 1:\n",
    "    sample = pdf.sample(size)\n",
    "    start = time.time()\n",
    "    sample.groupby('first').agg({'device_id': 'count'})\n",
    "    end = time.time()\n",
    "    size_time.append({\n",
    "        'size': size,\n",
    "        'time': end - start,\n",
    "    })\n",
    "    size = size // 100\n",
    "\n",
    "pdf_size_time = pd.DataFrame(size_time)\n",
    "pdf_size_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04922d-1709-4591-b827-01af9ac62c88",
   "metadata": {},
   "source": [
    "Even in-memory, the time it takes to aggregate the counts is not negligible, and it would obviously get worse with larger size.\n",
    "\n",
    "Depending on the available computing resources, this may vary. But after certain threashold, it exhibits a linear growth of time needed to perform the operation.\n",
    "\n",
    "Now let's try with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0029b07e-5a97-450f-9598-53a9f2576357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           device_id|first|\n",
      "+--------------------+-----+\n",
      "|bcdee560470ae56a1...|    b|\n",
      "|956f42d5039912ed8...|    9|\n",
      "|19aeae4388b84b560...|    1|\n",
      "|dbeaf78e828f91e91...|    d|\n",
      "|5c034a5a889a23c33...|    5|\n",
      "|b8da1f734f39e111b...|    b|\n",
      "|b4ec1b6bb3f4f6e99...|    b|\n",
      "|1d00c643689f121a3...|    1|\n",
      "|e0b305ca7695c3639...|    e|\n",
      "|c722e4d6182051df9...|    c|\n",
      "|aef11a579141779b8...|    a|\n",
      "|e0e317020994c8e66...|    e|\n",
      "|46e7a05a28877f956...|    4|\n",
      "|e38d8fbeca350bb14...|    e|\n",
      "|d895f5ad34b04d20c...|    d|\n",
      "|15db6050985fbbe2b...|    1|\n",
      "|d9dc4114fe8e772a8...|    d|\n",
      "|4021cb2d3c4bb893b...|    4|\n",
      "|fb55cd99222542f5f...|    f|\n",
      "|7dd4d2a488cc0127e...|    7|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('first', df.device_id.substr(0, 1))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402bc62-3733-46ca-b263-f6c722349e50",
   "metadata": {},
   "source": [
    "From above examples, we already know that Spark DataFrames, due to an entirely different and more complex mechanism compared to the more direct in-memory model that Pandas employs, the sampling process may be more time consuming. Therefore the runtime of the entire while loop logic may take much longer, but the aggregation portion is captured precisely like its Pandas counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb98a2bd-699b-4a08-975b-eb651cce5f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 ms, sys: 6.14 ms, total: 20.6 ms\n",
      "Wall time: 1min 21s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12000000</td>\n",
       "      <td>0.008463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119897</td>\n",
       "      <td>0.008105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1212</td>\n",
       "      <td>0.007846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>0.008244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       size      time\n",
       "0  12000000  0.008463\n",
       "1    119897  0.008105\n",
       "2      1212  0.007846\n",
       "3        19  0.008244"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "size_time = []\n",
    "count = df.count()\n",
    "size = count\n",
    "\n",
    "while size > 1:\n",
    "    sample = df.sample(size / count)\n",
    "    _count = sample.count()\n",
    "    start = time.time()\n",
    "    sample.groupby('first').agg({'device_id': 'count'})\n",
    "    end = time.time()\n",
    "    size_time.append({\n",
    "        'size': _count,\n",
    "        'time': end - start,\n",
    "    })\n",
    "    size = size // 100\n",
    "\n",
    "df_size_time = pd.DataFrame(size_time)\n",
    "df_size_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbc8ce-c65a-4519-a816-407b498df218",
   "metadata": {},
   "source": [
    "While it is significantly slower to bootstrap the samples, the aggregation shows a glimpse of Spark's true strength (or the distributed filesystem and the MapReduce model behind it). \n",
    "\n",
    "The time it takes to compute is not only much faster than its Pandas counterpart on larger sample sizes, but nearly uniform regardless of the given sample size. This scalability characteristic becomes more prominent, and also more essential as a tool, to process data when the sample size goes beyond a single machine's capacity, much like the situation that Google encountered in the early 2000s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aadd3bbb-cb34-4696-a5ad-618401c9862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locus example placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e0166-bb09-4d64-9ad9-7396d86a74b6",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "On a single machine, depending on the tasks, we can leverage techniques such as multiprocessing, multithreading, or coroutines to leverage underlying computing resources to expedite processing time.\n",
    "\n",
    "When the data size is much more than a single machine can handle, tools such as Apache Spark and Modin (and its underlying parallization abstractions Dask and Ray, all mentioned in [the previous part](./13-data-processing.ipynb)) become more essential to get the job done in a timely and cost-effective manner.\n",
    "\n",
    "For instance, at EQ Works, we leverage a cloud provider managed Spark service to run through terabytes to petabytes of data with variance of computational complexities in hours or minutes which would otherwise take days or even months on a single machine, even if the said machine _can_ hold such amount of data reliably.\n",
    "\n",
    "![EMR](https://user-images.githubusercontent.com/2837532/123005848-c89fed00-d384-11eb-8967-2e78faf718f2.png)\n",
    "\n",
    "## References\n",
    "\n",
    "* [Apache Spark](https://spark.apache.org/) and its [PySpark interface](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "* [Python Coroutines and Tasks](https://docs.python.org/3/library/asyncio-task.html)\n",
    "* [Part 11 - Work with SQL](./11-work-with-sql.ipynb)\n",
    "* [Part 12 - Generate Data](./12-generate-data.ipynb)\n",
    "* [Part 13 - The Power of Parallel Processing feat. multithreading and multiprocessing](./13-data-processing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbfdadc-fa51-4ee9-b27a-e3a5a27a8fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
