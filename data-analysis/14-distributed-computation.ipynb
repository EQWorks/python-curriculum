{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12adacc-007f-40fb-a86e-37ec50ac2b6f",
   "metadata": {},
   "source": [
    "# Distributed Computation\n",
    "\n",
    "## Quick Recap - Multiprocessing vs. Multithreading\n",
    "\n",
    "Multiprocessing can utilize multiple CPU cores, thus achieving a more authentic sense of parallel computation. However, multiprocessing suffers when they need to share a common memory space.\n",
    "\n",
    "On the other hand, multithreading can share a common memory space and achieve a more loose sense of parallel computation. As a result, multithreading is more like hyper-jumping through multiple queues (within the same process) while waiting for each particular thread's turn:\n",
    "1. When it is one specific thread's turn, it will acquire the Global Interpreter Lock (GIL) to control the memory space _and_ the CPU core until it finishes its designated computation or hits another \"busy-waiting\" block (such as I/O actions).\n",
    "2. Then, the loop releases the GIL and lets the jump proceed (context-switch) to the following thread.\n",
    "3. Rinse and repeat (until done).\n",
    "\n",
    "You can revisit [part 13](./13-data-processing.ipynb) for more details on this subject.\n",
    "\n",
    "### Asynchronous I/O Loops, and Multithreading's Little Brother - Coroutines\n",
    "\n",
    "Both multiprocessing and multithreading require dedicated hardware or operating system support. As software technologies mature, engineers started to explore capabilities within the application layer itself. As a result, the same conceptual model of multithreading gets a new interpretation within the programming stack (such as Python, or more appropriately, the CPython runtime), giving more direct control to the program within the runtime instead of relying on the OS mechanism to switch context between threads. The term [_Coroutine_](https://en.wikipedia.org/wiki/Coroutine), first coined in 1958, is a materialization of the concept of lightweight threads.\n",
    "\n",
    "Both multithreading and coroutine techniques are suitable for I/O focused tasks, such as reading files from a disk or making HTTP requests. However, coroutines tend to require less computing resource overhead than threads by giving up some performance benefit from tapping into the more OS-native mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62e3a796-c985-4659-a933-2d4dde6a1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import requests\n",
    "\n",
    "\n",
    "# 5 requests, each with delays ranging from 1-3 seconds\n",
    "reqs = [\n",
    "    f'https://httpbin.org/delay/{random.randint(1, 3)}'\n",
    "    for _ in range(5)\n",
    "]\n",
    "\n",
    "def get_sync():\n",
    "    all_data = []\n",
    "    for req in reqs:\n",
    "        res = requests.get(req)\n",
    "        all_data.append(res.json())\n",
    "\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8483a5c-e857-41d8-8191-14e5ff96132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84.2 ms, sys: 6.88 ms, total: 91.1 ms\n",
      "Wall time: 11.7 s\n",
      "{'args': {}, 'data': '', 'files': {}, 'form': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.25.1', 'X-Amzn-Trace-Id': 'Root=1-60d2651e-364946872247cadd59364ddd'}, 'origin': '107.179.188.69', 'url': 'https://httpbin.org/delay/1'} 5\n"
     ]
    }
   ],
   "source": [
    "%time res = get_sync()\n",
    "print(res[-1], len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c107f433-9bc4-4b0f-9dd1-77603dfaa07a",
   "metadata": {},
   "source": [
    "In Python, [the `asyncio` module](https://docs.python.org/3/library/asyncio.html) provides a standard set of APIs for its users to utilize coroutines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b010385-d4ff-4870-9e0e-4f1164dc465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import aiohttp\n",
    "\n",
    "\n",
    "async def get(session, url):\n",
    "    res = await session.request('GET', url=url)\n",
    "    data = await res.json()\n",
    "    return data\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for req in reqs:\n",
    "            tasks.append(get(session, url=req))\n",
    "\n",
    "        all_data = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a7ffa8-f260-4c4e-ae5f-ac3d4c6eb658",
   "metadata": {},
   "source": [
    "There are two caveats about utilizing `asyncio` in the Jupyter Notebook environment:\n",
    "1. We cannot use the %time magic command for async functions.\n",
    "2. We cannot initiate an explicit event loop (it's already in one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f338e2f-1951-4558-bf6d-531bd7becb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3 seconds\n",
      "{'args': {}, 'data': '', 'files': {}, 'form': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Host': 'httpbin.org', 'User-Agent': 'Python/3.8 aiohttp/3.7.4.post0', 'X-Amzn-Trace-Id': 'Root=1-60d26534-7369e48c66ce404f66172d19'}, 'origin': '107.179.188.69', 'url': 'https://httpbin.org/delay/1'} 5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# loop = asyncio.get_event_loop()\n",
    "# loop.run_until_complete(main())\n",
    "start = time.time()\n",
    "res = await main()\n",
    "print(round(time.time() - start, 1), 'seconds')\n",
    "print(res[-1], len(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e866076f-3616-4e9b-8b48-2c07c6f5f627",
   "metadata": {},
   "source": [
    "Let's use the responses to verify what the delays were in the API calls by using functional techniques `map()` and `reduce()` to extract delays (in seconds) and sum them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "920e2588-47d2-44a4-a979-2f14739fe203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 3, 2, 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delays = [*map(lambda url: int(url.split('/')[-1]), reqs)]\n",
    "delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "658e2b6b-5d19-44eb-a486-b6841d3bf1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "total_delay = reduce(lambda left, right: left + right, [*delays], 0)\n",
    "total_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba3091-88b4-46c6-a431-09afb9da6873",
   "metadata": {},
   "source": [
    "The theoretical total delay matches our observation from the synchronous process, while the _maximum_ from individual delays matches our observation from the asynchronous process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12faf4-d0ea-4921-b5e1-01a864f7aa8b",
   "metadata": {},
   "source": [
    "## MapReduce - Distributing Computing Resources\n",
    "\n",
    "Recall one of the most critical disadvantages that multiprocessing - the lack of a shared memory space. This particular constraint makes attempt to perform parrallel computation across multiple cores on a single machine often less desirable. One workaround of such issue is to leverage a datastore (such as files or databases backed by harddrives) as an inter-process data pool so that multiple processes can simultaneously read and write to it, such as a local SQLite database, as demonstrated in [a previous part](./11-work-with-sql.ipynb).\n",
    "\n",
    "If we extend this problem to a larger scale, where not only the dataset we want to work with exceed the available memory space, but also impossible to efficiently store (or at all) on the disk of a single machine, then we need to revisit viable solutions.\n",
    "\n",
    "The MapReduce model is a _divide and conquer_ strategy applying and extending the functional programming concepts of `map()` and `reduce()`, where a large dataset is dissected and distributed through a mapping procedure onto a multitude of \"commodity\" server nodes to parrallize the computation of the smaller portion, then reducing the resulting subsets back to less and less nodes until the final outcome is completed.\n",
    "\n",
    "This model allows batch data processing to have near infinite capacity, and a relatively cost-effective way to speed up the process.\n",
    "\n",
    "It is worth knowing that the recent development of cheaper and faster harddrives, especially the more wide-spread adoption of SSDs (solid-state drive), plays a vital role in enabling distributed computation.\n",
    "\n",
    "The MapReduce model was first pioneered [by Google](https://research.google/pubs/pub62/) in 2004 to resolve the practical problem of exponentially growing dataset for computing their search indexes. Then many have adopted and contributed toward the technology's development and evolution through the open-source community.\n",
    "\n",
    "### Apache Spark™\n",
    "\n",
    "Apache Spark is such an open-source framework that came around 2014 (10 years after the initial MapReduce research paper) that provides an elegant and unified abstraction to enable large-scale data processing that can efficiently utilize from multiple-cores of a single machine to \"multiple-clouds\".\n",
    "\n",
    "For the sake of simplicity, we will demonstrate through its usage on a single machine. First, obtain the number of CPU cores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d2d71cc-fdac-4c6e-985f-8a7f784b37a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "CORES = multiprocessing.cpu_count()\n",
    "CORES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62ec5e4-c646-4357-b372-78c5d1d4e8e7",
   "metadata": {},
   "source": [
    "Borrowing from [another previous part](./12-generate-data.ipynb), where we attempted to generate random and hashed device IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab89d25-26a1-4bf9-841b-23005c22b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostly from Part 12 - Generate data\n",
    "from uuid import uuid4\n",
    "from hashlib import sha1\n",
    "\n",
    "\n",
    "def gen_device_ids(count: int = 20_000) -> list:\n",
    "    device_ids = []\n",
    "    for _ in range(count):\n",
    "        device_ids.append(str(uuid4()))\n",
    "    # hash\n",
    "    return [sha1(x.encode()).hexdigest() for x in device_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d5ea06-9dba-44f2-a653-9c2856366b4d",
   "metadata": {},
   "source": [
    "Let's generate a relatively large batch of device IDs, say 1 million times the number of `CORES`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a7c6a45-9867-4cff-9f28-c58b3025ac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.9 s, sys: 5.31 s, total: 57.2 s\n",
      "Wall time: 57.4 s\n"
     ]
    }
   ],
   "source": [
    "%time device_ids = gen_device_ids(count=1_000_000 * CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8c3ce-3642-46b8-800b-418b0c753344",
   "metadata": {},
   "source": [
    "Due to the iterative single process, the time it takes is quite long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58a5abe5-5615-46c8-947d-e04def2c91cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['febf7697c700bcf0e19c28b1694d9ade9daa94a4',\n",
       " '9b28c48c527505f964db5492da238af2dc49bf62',\n",
       " '86709d1d565d531f32b07f563826befd1a219690',\n",
       " '39c4a141f0ac58b7f6369af3fe23e82501b60f35',\n",
       " '180702a758db917758cd828918494c01a97e8757',\n",
       " '9af5eef60b00e5cfb2cf38f8fe3474ed09dd9a79',\n",
       " '0f131b59e1eeaa1afe51734b5b064754800e056c',\n",
       " '1f16df80860ed088b47e60da7c047e43cbd760e8',\n",
       " '2a7ae71c4e87825fab7333431cf8c91b6457bae4',\n",
       " '1afec761c3238d688b46be71a8e76b315b14dbf2']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time device_ids[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481733f-e3f3-4678-a2d9-7af6a53e8b54",
   "metadata": {},
   "source": [
    "But since the list is already in memory, it takes very little time to read them out.\n",
    "\n",
    "Let's try the same device ID generation with Spark in a distributed manner. We will initiate a Spark session with a local \"master\" that takes advantage of the number of `CORES` we have obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30282e77-5efc-45fb-8753-e1a006c4051c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://runzhoudembp.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[12]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1479ead60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(f'local[{CORES}]').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d1fb0-1a29-45a8-a632-945ed4a6623a",
   "metadata": {},
   "source": [
    "The Spark UI which runs on the `localhost` is handy for monitoring and more.\n",
    "\n",
    "![Spark UI](https://user-images.githubusercontent.com/2837532/122995953-46a9c700-d378-11eb-84a7-50917d34b7be.png)\n",
    "\n",
    "The first approach involves a core concept and building block of Spark which is known as Resilient Distributed Datasets (`RDD`s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb80f2ce-34df-4ab1-b043-4e6585ada283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.78 ms, sys: 2.07 ms, total: 3.85 ms\n",
      "Wall time: 193 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def mapper(count):\n",
    "    return [(d,) for d in gen_device_ids(count)]\n",
    "\n",
    "# a list of [1_000_000, ..., 1_000_000], where the length is the number of CORES\n",
    "counts = [1_000_000] * CORES\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(counts).flatMap(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75e05c8-c9ee-46e5-8301-0f3dbe2c15b5",
   "metadata": {},
   "source": [
    "The action takes a context from the available Spark session, schedule to parallelize over a list of 1 million counts, where the list length is the number of `CORES`. Then we instruct the parallelization to map the list over a `mapper` function that takes the individual count and generate a list of device IDs (in the form of a single element tuple). The `.flatMap` method is a convenient layer to ensure the resulting dataset is flattened as a single-tier list of device IDs, instead of a list of (number of `CORES`) lists.\n",
    "\n",
    "Notice the time it takes to schedule is negligible, as Spark takes a _lazy_ approach to preserve computing resources until the computation is really needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0df7f70-b1ee-44fe-af21-89c21df0221a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.79 ms, sys: 2.83 ms, total: 8.62 ms\n",
      "Wall time: 5.81 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('573bcd3a6fc34384927103af362376d5b0f4b8b4',),\n",
       " ('3a6ddce8cb2f313838fa45b5ad43589692b56031',),\n",
       " ('1e813510fe0b256af4efc38fd402a43ce83084bb',),\n",
       " ('12522da8b4b0fc4743bea819d87361219757d1b5',),\n",
       " ('ef92f7f3fe7649a7e6bc075f4087edab14dd56a0',),\n",
       " ('4a2f9b4ea5b39cc3d54ee0e14ffc53fa767a5250',),\n",
       " ('30493527bc351bca3a5846f65e780b6096b820e7',),\n",
       " ('30d2fc83f4ddc3daa040613a62ccac4ce6b7aced',),\n",
       " ('5ab40e264de56d041f7ad57c8ecfd3faabc4ced6',),\n",
       " ('de8af809e8bdb2658bee7c622709af5c0b0d318d',)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22793df6-78be-48e4-8b5c-87964f7230fc",
   "metadata": {},
   "source": [
    "Indeed, when we instruct to take the first 10 values from the RDD, Spark would actually start the computation (and more). If we go back to the Spark UI, it may paint a better picture on what Spark has done under the hood.\n",
    "\n",
    "![RDD take](https://user-images.githubusercontent.com/2837532/122997519-16fbbe80-d37a-11eb-8704-5c520019e161.png)\n",
    "\n",
    "Conceptually, the workflow is as the following:\n",
    "1. Generate the device IDs leveraging multiple `CORES` in parallel.\n",
    "2. Persist device IDs as RDD into distributed blocks on disk. They are also replicated automatically across available distributions to support further parallelized processes. Spark also records other necessary metadata such as order of values to ensure that computations that require strict ordering do not get affected.\n",
    "3. Draw the first 10 values from the RDD files and populates into the Python process that runs this Notebook.\n",
    "\n",
    "This means that while it is significantly faster to generate the device IDs compare to the single process iterative approach, it is also much slower to read it out as it involves a more complex trip to read from files while ensuring the order of values are intact.\n",
    "\n",
    "Spark also has a built-in DataFrame implementation on top of the RDDs, providing neat abstractions such as SQL queries and more, similar to Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79918937-4a6b-43ee-a35a-ad09406581d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 358 ms, sys: 40.4 ms, total: 398 ms\n",
      "Wall time: 6.45 s\n"
     ]
    }
   ],
   "source": [
    "%time df = rdd.toDF(['device_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80978d82-77ca-4c72-a07e-3d107e6208c3",
   "metadata": {},
   "source": [
    "This step reveals the trade-off more prominently, where the time it takes to operate on distributed dataset can be much more time consuming than its in-memory counterpart, where we can take the in-memory `list` of `device_ids` and convert it into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "668cfff9-de2a-41b4-a72d-fa13cc48d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14282bfc-a1f2-41a7-8e23-95a6e81a0804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 552 ms, sys: 196 ms, total: 749 ms\n",
      "Wall time: 747 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>febf7697c700bcf0e19c28b1694d9ade9daa94a4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9b28c48c527505f964db5492da238af2dc49bf62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86709d1d565d531f32b07f563826befd1a219690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39c4a141f0ac58b7f6369af3fe23e82501b60f35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180702a758db917758cd828918494c01a97e8757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999995</th>\n",
       "      <td>fd2a9f5bf95a26704c8dacd413d507c30e63d9a5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999996</th>\n",
       "      <td>f5fadf65f12df3da07a871722a4f44af2cfb4be8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999997</th>\n",
       "      <td>355434966437796f8b036f177b057ff609f2ec1e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999998</th>\n",
       "      <td>9a15ae5f0b98ac9a0aaa043a190840bdbcf0d729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999999</th>\n",
       "      <td>d2a64bd14bfec8c46578bb235ca15aadc89198b3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         device_id\n",
       "0         febf7697c700bcf0e19c28b1694d9ade9daa94a4\n",
       "1         9b28c48c527505f964db5492da238af2dc49bf62\n",
       "2         86709d1d565d531f32b07f563826befd1a219690\n",
       "3         39c4a141f0ac58b7f6369af3fe23e82501b60f35\n",
       "4         180702a758db917758cd828918494c01a97e8757\n",
       "...                                            ...\n",
       "11999995  fd2a9f5bf95a26704c8dacd413d507c30e63d9a5\n",
       "11999996  f5fadf65f12df3da07a871722a4f44af2cfb4be8\n",
       "11999997  355434966437796f8b036f177b057ff609f2ec1e\n",
       "11999998  9a15ae5f0b98ac9a0aaa043a190840bdbcf0d729\n",
       "11999999  d2a64bd14bfec8c46578bb235ca15aadc89198b3\n",
       "\n",
       "[12000000 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pdf = pd.DataFrame(device_ids, columns=['device_id'])\n",
    "\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80250bd-5186-48cc-8fa1-d68d777000dc",
   "metadata": {},
   "source": [
    "Bearing the same understanding, the Spark DataFrame would be slower to read, due to the round-trip it takes to the distributed disk blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5eb420e0-3da9-438a-ba00-769fd74c20bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           device_id|\n",
      "+--------------------+\n",
      "|2e9644239b7cc33e8...|\n",
      "|251f4722f37e2a937...|\n",
      "|ac9e04262f0c15b46...|\n",
      "|a76b13cf7f71869a1...|\n",
      "|6da357e917f3da496...|\n",
      "|4cc9f4a9bceae8baa...|\n",
      "|4774aaad84c9ce2ee...|\n",
      "|9c6c39454d0d0769c...|\n",
      "|22434c0bf67e5f9cc...|\n",
      "|10e1aa81878ea6e60...|\n",
      "|4df5812cd9d2e5d43...|\n",
      "|05f94b9226b0c83a6...|\n",
      "|3cd56c7020ccfa814...|\n",
      "|b589f8a926c3115e1...|\n",
      "|fb042723528fc5f4f...|\n",
      "|50e1e59e40b3d3361...|\n",
      "|7d705e2801fd5d065...|\n",
      "|43234eb1c0f7ead35...|\n",
      "|6a108f0b194f77b4a...|\n",
      "|fbd4fc33e217e8128...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 1.15 ms, sys: 996 µs, total: 2.15 ms\n",
      "Wall time: 5.56 s\n"
     ]
    }
   ],
   "source": [
    "%time df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e93dcb6-db18-42ec-b2ba-b7773b3c00b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 396 ms, sys: 8.18 ms, total: 404 ms\n",
      "Wall time: 403 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device_id    12000000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "418fede8-ede8-4edc-ad5d-db1c7e198864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 ms, sys: 1.83 ms, total: 4.12 ms\n",
      "Wall time: 16.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe9ebd-f9db-4f97-960f-25eecd7d328c",
   "metadata": {},
   "source": [
    "Taking the full count can be more significant of a difference. Spark would attempt to perform underlying optimization to not scan the full range of dataset when invoking partial readings such as `rdd.take()` or `df.show()`, but suffers a full-range scan when it needs to count the accurate number of items, hence the much longer duration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271355d8-1cf8-4a91-8a3f-8b64f3904888",
   "metadata": {},
   "source": [
    "Spark DataFrames can also be created directly, or through the help of concatinating existing Pandas DataFrames.\n",
    "\n",
    "Below is an alternative approach to perform the device ID generation task by leveraging the robust support of Spark's native support to interface with Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bca3108-7df8-4744-adcb-26a2e221a769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.5 ms, sys: 5.25 ms, total: 48.7 ms\n",
      "Wall time: 218 ms\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(i,) for i in range(CORES)], ['cluster'])\n",
    "\n",
    "def _gen(df):\n",
    "    device_ids = gen_device_ids(count=1_000_000)\n",
    "    pdf = pd.DataFrame(device_ids, columns=['device_id'])\n",
    "    pdf['cluster'] = df['cluster']\n",
    "    return pdf.reset_index()\n",
    "\n",
    "def gen_device_ids_udf(df):\n",
    "    output = []\n",
    "    for _, row in df.iterrows():\n",
    "        pdf = _gen(df)\n",
    "        output.append(pdf)\n",
    "\n",
    "    return pd.concat(output)\n",
    "\n",
    "\n",
    "schema = 'index long, cluster long, device_id string'\n",
    "%time df = df.groupby('cluster').applyInPandas(gen_device_ids_udf, schema=schema).drop('cluster', 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe3c37-9cf6-439a-8b57-fac23ba580f5",
   "metadata": {},
   "source": [
    "The similar parallelization principle from the RDD approach applies here, with a _hack_ around the mechanism of `.groupby()` which enables the parallelization against the number fo `CORES`.\n",
    "\n",
    "Also similarly, the scheduling of the task does not take much, as we have yet to request for actual access to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d620162-9d26-46d5-871d-82b987b73541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           device_id|\n",
      "+--------------------+\n",
      "|b3624be1446a9c651...|\n",
      "|620a012fe70bbff6b...|\n",
      "|02c22a78668015932...|\n",
      "|50c3e622a52400eab...|\n",
      "|246247ae209b744bb...|\n",
      "|0d0e0bb66b184f47c...|\n",
      "|70e0e67aac25517c6...|\n",
      "|7a87d9a32ae988dd8...|\n",
      "|9ac88faf90644db27...|\n",
      "|260e1206125cbb6c8...|\n",
      "|cd646c9c613f0902d...|\n",
      "|af26d6ed80e434ecc...|\n",
      "|a967561fc441f1951...|\n",
      "|dd6a428cc42ed5a49...|\n",
      "|032ffa42748cce517...|\n",
      "|d8037c4744e21b142...|\n",
      "|6f2052eb2db8868cb...|\n",
      "|c0dae01dfadaf24ec...|\n",
      "|4d5c20e70117653df...|\n",
      "|4bc967f393258b90b...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 3.3 ms, sys: 2.82 ms, total: 6.12 ms\n",
      "Wall time: 6.7 s\n"
     ]
    }
   ],
   "source": [
    "%time df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab32a57-cb9e-4c90-b769-0a5cdfd43f1d",
   "metadata": {},
   "source": [
    "Since this approach involves a more pronounced mapping process abstracted by the `.groupby` method, underlying workflow can be a bit more interesting to observe:\n",
    "\n",
    "![group by](https://user-images.githubusercontent.com/2837532/123001100-5b895900-d37e-11eb-8b2a-db3fa40caaba.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5144c54a-b74a-43d4-94fc-953e27d84f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.8 ms, sys: 9.95 ms, total: 25.8 ms\n",
      "Wall time: 15.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908a266-8738-43b2-b96f-22cf114c11d8",
   "metadata": {},
   "source": [
    "Read access and the time consumption behaviours are within our expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c4298-1a42-426d-a252-8466de901e80",
   "metadata": {},
   "source": [
    "Let's attempt to perform some actual analysis of the overall dataset, such as counting the number of device IDs by their first characters.\n",
    "\n",
    "We'll start with the Pandas DataFrame, which is in-memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d87e7dc-8cb7-4dfc-8810-993e34d40848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>device_id</th>\n",
       "      <th>first</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>febf7697c700bcf0e19c28b1694d9ade9daa94a4</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9b28c48c527505f964db5492da238af2dc49bf62</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86709d1d565d531f32b07f563826befd1a219690</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39c4a141f0ac58b7f6369af3fe23e82501b60f35</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180702a758db917758cd828918494c01a97e8757</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999995</th>\n",
       "      <td>fd2a9f5bf95a26704c8dacd413d507c30e63d9a5</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999996</th>\n",
       "      <td>f5fadf65f12df3da07a871722a4f44af2cfb4be8</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999997</th>\n",
       "      <td>355434966437796f8b036f177b057ff609f2ec1e</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999998</th>\n",
       "      <td>9a15ae5f0b98ac9a0aaa043a190840bdbcf0d729</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999999</th>\n",
       "      <td>d2a64bd14bfec8c46578bb235ca15aadc89198b3</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         device_id first\n",
       "0         febf7697c700bcf0e19c28b1694d9ade9daa94a4     f\n",
       "1         9b28c48c527505f964db5492da238af2dc49bf62     9\n",
       "2         86709d1d565d531f32b07f563826befd1a219690     8\n",
       "3         39c4a141f0ac58b7f6369af3fe23e82501b60f35     3\n",
       "4         180702a758db917758cd828918494c01a97e8757     1\n",
       "...                                            ...   ...\n",
       "11999995  fd2a9f5bf95a26704c8dacd413d507c30e63d9a5     f\n",
       "11999996  f5fadf65f12df3da07a871722a4f44af2cfb4be8     f\n",
       "11999997  355434966437796f8b036f177b057ff609f2ec1e     3\n",
       "11999998  9a15ae5f0b98ac9a0aaa043a190840bdbcf0d729     9\n",
       "11999999  d2a64bd14bfec8c46578bb235ca15aadc89198b3     d\n",
       "\n",
       "[12000000 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf['first'] = pdf.device_id.astype(str).str[0]\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6dd9960-f20e-4bd8-9d6e-adbc6a3d74c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12000000</td>\n",
       "      <td>2.354549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120000</td>\n",
       "      <td>0.030982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200</td>\n",
       "      <td>0.002792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.001873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       size      time\n",
       "0  12000000  2.354549\n",
       "1    120000  0.030982\n",
       "2      1200  0.002792\n",
       "3        12  0.001873"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "size_time = []\n",
    "size = len(pdf)\n",
    "\n",
    "while size > 1:\n",
    "    sample = pdf.sample(size)\n",
    "    start = time.time()\n",
    "    sample.groupby('first').agg({'device_id': 'count'})\n",
    "    end = time.time()\n",
    "    size_time.append({\n",
    "        'size': size,\n",
    "        'time': end - start,\n",
    "    })\n",
    "    size = size // 100\n",
    "\n",
    "pdf_size_time = pd.DataFrame(size_time)\n",
    "pdf_size_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04922d-1709-4591-b827-01af9ac62c88",
   "metadata": {},
   "source": [
    "Even in-memory, the time it takes to aggregate the counts is not negligible, and it would obviously get worse with larger size.\n",
    "\n",
    "Depending on the available computing resources, this may vary. But after certain threashold, it exhibits a linear growth of time needed to perform the operation.\n",
    "\n",
    "Now let's try with Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0029b07e-5a97-450f-9598-53a9f2576357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           device_id|first|\n",
      "+--------------------+-----+\n",
      "|8819657ee2ae716ae...|    8|\n",
      "|775fff96ef0b57250...|    7|\n",
      "|efc4765433e9c2b9f...|    e|\n",
      "|e857d474fca33eeda...|    e|\n",
      "|3af20fda97011257e...|    3|\n",
      "|58f0196ec6c39ca0d...|    5|\n",
      "|b71299272dd289954...|    b|\n",
      "|df88e50b836d6ec62...|    d|\n",
      "|564563cae645631ca...|    5|\n",
      "|a90df3138c0c90656...|    a|\n",
      "|3dfcafba601f0f840...|    3|\n",
      "|0674924f56cf0a573...|    0|\n",
      "|ad107f4631dd6b533...|    a|\n",
      "|8622654408bf70128...|    8|\n",
      "|ef032047843cae893...|    e|\n",
      "|d6601aeb82b8cc91b...|    d|\n",
      "|e37e34c064b16535d...|    e|\n",
      "|24114557b4a7e9b10...|    2|\n",
      "|30dc00cf907d5a3b7...|    3|\n",
      "|097129d6e3e6ae525...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('first', df.device_id.substr(0, 1))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402bc62-3733-46ca-b263-f6c722349e50",
   "metadata": {},
   "source": [
    "From above examples, we already know that Spark DataFrames, due to an entirely different and more complex mechanism compared to the more direct in-memory model that Pandas employs, the sampling process may be more time consuming. Therefore the runtime of the entire while loop logic may take much longer, but the aggregation portion is captured precisely like its Pandas counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb98a2bd-699b-4a08-975b-eb651cce5f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 183 ms, sys: 111 ms, total: 294 ms\n",
      "Wall time: 2min 29s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12000000</td>\n",
       "      <td>17.799956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>119741</td>\n",
       "      <td>16.251669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1210</td>\n",
       "      <td>17.157911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>15.165370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       size       time\n",
       "0  12000000  17.799956\n",
       "1    119741  16.251669\n",
       "2      1210  17.157911\n",
       "3        10  15.165370"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "size_time = []\n",
    "count = df.count()\n",
    "size = count\n",
    "\n",
    "while size > 1:\n",
    "    sample = df.sample(size / count)\n",
    "    _count = sample.count()\n",
    "    start = time.time()\n",
    "    sample.groupby('first').agg({'device_id': 'count'}).collect()  # collect is used to emulate full data scan/process\n",
    "    end = time.time()\n",
    "    size_time.append({\n",
    "        'size': _count,\n",
    "        'time': end - start,\n",
    "    })\n",
    "    size = size // 100\n",
    "\n",
    "df_size_time = pd.DataFrame(size_time)\n",
    "df_size_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbc8ce-c65a-4519-a816-407b498df218",
   "metadata": {},
   "source": [
    "While it is significantly slower to bootstrap the samples, the aggregation shows a glimpse of Spark's true strength (or the distributed filesystem and the MapReduce model behind it). \n",
    "\n",
    "The time it takes to compute the aggregation is nearly uniform regardless of the given sample size. This scalability characteristic becomes more prominent, and also more essential as a tool, to process data when the sample size goes beyond a single machine's capacity, much like the situation that Google encountered in the early 2000s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aadd3bbb-cb34-4696-a5ad-618401c9862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locus example placeholder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86e0166-bb09-4d64-9ad9-7396d86a74b6",
   "metadata": {},
   "source": [
    "## Remarks\n",
    "\n",
    "On a single machine, depending on the tasks, we can leverage techniques such as multiprocessing, multithreading, or coroutines to leverage underlying computing resources to expedite processing time.\n",
    "\n",
    "When the data size is much more than a single machine can handle, tools such as Apache Spark and Modin (and its underlying parallization abstractions Dask and Ray, all mentioned in [the previous part](./13-data-processing.ipynb)) become more essential to get the job done in a timely and cost-effective manner.\n",
    "\n",
    "For instance, at EQ Works, we leverage a cloud provider managed Spark service to run through terabytes to petabytes of data with variance of computational complexities in hours or minutes which would otherwise take days or even months on a single machine, even if the said machine _can_ hold such amount of data reliably.\n",
    "\n",
    "![EMR](https://user-images.githubusercontent.com/2837532/123005848-c89fed00-d384-11eb-8967-2e78faf718f2.png)\n",
    "\n",
    "## References\n",
    "\n",
    "* [Apache Spark](https://spark.apache.org/) and its [PySpark interface](https://spark.apache.org/docs/latest/api/python/index.html)\n",
    "* [Python Coroutines and Tasks](https://docs.python.org/3/library/asyncio-task.html)\n",
    "* [Part 11 - Work with SQL](./11-work-with-sql.ipynb)\n",
    "* [Part 12 - Generate Data](./12-generate-data.ipynb)\n",
    "* [Part 13 - The Power of Parallel Processing feat. multithreading and multiprocessing](./13-data-processing.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
